{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The model has 14153746 learnable parameters\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from models.ResModel import ResModel\n",
    "from models.Model import Model\n",
    "from models.JointModel import JointModel\n",
    "from models.MonoSF import MonoSceneFlow\n",
    "from losses import Loss\n",
    "from datasets.kitti_2015_train import KITTI_2015_MonoSceneFlow\n",
    "from augmentations import Augmentation_Resize_Only, Augmentation_SceneFlow\n",
    "from collections import OrderedDict\n",
    "from datasets.kitti_raw_monosf import KITTI_Odom_Test\n",
    "import numpy as np\n",
    "\n",
    "class Args:\n",
    "    cuda = True\n",
    "    use_bn = False\n",
    "    momentum = 0.9\n",
    "    beta = 0.999\n",
    "    weight_decay=0.0\n",
    "    train_exp_mask=False\n",
    "    train_census_mask=True\n",
    "    model_name = \"joint\"\n",
    "    encoder_name='pwc'\n",
    "    disp_pts_w = 0.0\n",
    "    flow_pts_w = 0.2\n",
    "    sf_sm_w = 200\n",
    "    disp_sm_w = 0.2\n",
    "    do_pose_c2f = False\n",
    "    ssim_w = 0.85\n",
    "    disp_smooth_w = 0.1\n",
    "    mask_reg_w = 0.0\n",
    "    num_examples = 200\n",
    "    static_cons_w = 0.0\n",
    "    mask_cons_w = 0.0\n",
    "    mask_sm_w = 0.0\n",
    "    batch_size=1\n",
    "    flow_diff_thresh=1e-3\n",
    "    pt_encoder=False\n",
    "    num_scales = 4\n",
    "    evaluation=True\n",
    "    use_disp_min=False\n",
    "    flow_reduce_mode=\"sum\"\n",
    "    apply_flow_mask = False\n",
    "    apply_mask = True \n",
    "    mask_thresh=0.3\n",
    "    use_bottleneck=True\n",
    "    flow_sm_w = 200\n",
    "    flow_cycle_w = 0.0\n",
    "    use_static_mask=False\n",
    "    disp_lr_w = 0.1\n",
    "\n",
    "args = Args()\n",
    "\n",
    "model = JointModel(args).cuda()\n",
    "\n",
    "loss = Loss(args).cuda()\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"The model has {num_params} learnable parameters\")\n",
    "\n",
    "state_dict = torch.load('pretrained/debug.ckpt')['model']\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:]\n",
    "    new_state_dict[name] = v\n",
    "model.load_state_dict(new_state_dict)\n",
    "# model = model.eval()\n",
    "\n",
    "dataset_09 = KITTI_Odom_Test(args, \"/external/datasets/kitti_data_jpg/\", \"09\")\n",
    "dataloader_09 = DataLoader(dataset_09, shuffle=False, batch_size=1, pin_memory=True)\n",
    "dataset_10 = KITTI_Odom_Test(args, \"/external/datasets/kitti_data_jpg/\", \"10\")\n",
    "dataloader_10 = DataLoader(dataset_10, shuffle=False, batch_size=1, pin_memory=True)\n",
    "\n",
    "aug = Augmentation_Resize_Only(args).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 795/795 [07:42<00:00,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Trajectory error: 0.008, std: 0.008\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from https://github.com/tinghuiz/SfMLearner\n",
    "def compute_ate(gtruth_xyz, pred_xyz):\n",
    "    alignment_error = pred_xyz - gtruth_xyz\n",
    "    rmse = np.sqrt(sum(alignment_error ** 2)) / gtruth_xyz.shape[0]\n",
    "    return rmse\n",
    "\n",
    "seq_09_ates = []\n",
    "seq_09_res = []\n",
    "for i, data in enumerate(tqdm(dataloader_09)):\n",
    "    with torch.no_grad():\n",
    "        # Get input and target tensor keys\n",
    "        input_keys = list(filter(lambda x: \"input\" in x, data.keys()))\n",
    "        target_keys = list(filter(lambda x: \"target\" in x, data.keys()))\n",
    "        tensor_keys = input_keys + target_keys\n",
    "        \n",
    "        # Possibly transfer to Cuda\n",
    "        for k, v in data.items():\n",
    "            if k in tensor_keys:\n",
    "                data[k] = v.cuda(non_blocking=True)\n",
    "                \n",
    "        aug_data = aug(data)\n",
    "        out = model(aug_data)\n",
    "        pose_t = out['pose_b'][0][:, :3, 3].double().cpu()\n",
    "        gt_t = data['target_pose'][:, :3, 3].double().cpu()\n",
    "        \n",
    "        pose_R = out['pose_b'][0][:, :3, :3].double().cpu()[0]\n",
    "        gt_R = data['target_pose'][:, :3, :3].double().cpu()[0]\n",
    "        \n",
    "        R = torch.matmul(gt_R, torch.inverse(pose_R))\n",
    "        s = np.linalg.norm([R[0, 1]-R[1, 0],\n",
    "                            R[1, 2]-R[2, 1],\n",
    "                            R[0, 2]-R[2, 0]])\n",
    "        c = np.trace(R) - 1\n",
    "        RE = np.arctan2(s, c)\n",
    "        \n",
    "        ate = compute_ate(gt_t, pose_t)\n",
    "        seq_09_ates.append(np.array(ate))\n",
    "        seq_09_res.append(RE)\n",
    "        \n",
    "seq_09_ates = np.array(seq_09_ates)\n",
    "seq_09_res = np.array(seq_09_res)\n",
    "print(\"\\n   Trajectory error: {:0.3f}, std: {:0.3f}\\n\".format(np.mean(seq_09_ates), np.std(seq_09_ates)))\n",
    "print(\"\\n   Rotation error: {:0.3f}, std: {:0.3f}\\n\".format(np.mean(seq_09_res), np.std(seq_09_res)))\n",
    "\n",
    "# save_path = os.path.join(opt.load_weights_folder, \"poses.npy\")\n",
    "# np.save(save_path, pred_poses)\n",
    "# print(\"-> Predictions saved to\", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 26/1200 [00:09<07:27,  2.63it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-78fce5c2a438>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0maug_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mpose_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pose_b'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mgt_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target_pose'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/env_sf/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/research/sceneflow/motionflow/models/JointModel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_dict)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;31m## Left\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0moutput_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pwc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_l1_aug'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_l2_aug'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_k_l1_aug'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_k_l2_aug'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;31m## Right\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/research/sceneflow/motionflow/models/JointModel.py\u001b[0m in \u001b[0;36mrun_pwc\u001b[0;34m(self, input_dict, x1_raw, x2_raw, k1, k2)\u001b[0m\n\u001b[1;32m    165\u001b[0m                     \u001b[0mx2_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflow_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisp_l2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_l2\u001b[0m\u001b[0;34m,\u001b[0m      \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpose_b_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow_estimators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout_corr_relu_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 \u001b[0mpose_mat_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpose_vec2mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpose_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m                 \u001b[0mpose_mat_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minvert_pose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpose_mat_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/research/sceneflow/motionflow/utils/inverse_warp.py\u001b[0m in \u001b[0;36mpose_vec2mat\u001b[0;34m(vec, rotation_mode)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mrot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrotation_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'euler'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mrot_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meuler2mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrot\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B, 3, 3]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mrotation_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quat'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mrot_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquat2mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrot\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B, 3, 3]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/research/sceneflow/motionflow/utils/inverse_warp.py\u001b[0m in \u001b[0;36meuler2mat\u001b[0;34m(angle)\u001b[0m\n\u001b[1;32m    105\u001b[0m     ymat = torch.stack([cosy, zeros,  siny,\n\u001b[1;32m    106\u001b[0m                         \u001b[0mzeros\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                         -siny, zeros,  cosy], dim=1).view(B, 3, 3)\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mcosx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seq_10_ates = []\n",
    "seq_10_res = []\n",
    "for i, data in enumerate(tqdm(dataloader_10)):\n",
    "    with torch.no_grad():\n",
    "        # Get input and target tensor keys\n",
    "        input_keys = list(filter(lambda x: \"input\" in x, data.keys()))\n",
    "        target_keys = list(filter(lambda x: \"target\" in x, data.keys()))\n",
    "        tensor_keys = input_keys + target_keys\n",
    "        \n",
    "        # Possibly transfer to Cuda\n",
    "        for k, v in data.items():\n",
    "            if k in tensor_keys:\n",
    "                data[k] = v.cuda(non_blocking=True)\n",
    "                \n",
    "        aug_data = aug(data)\n",
    "        out = model(aug_data)\n",
    "        pose_t = out['pose_b'][0][:, :3, 3].double().cpu()\n",
    "        gt_t = data['target_pose'][:, :3, 3].double().cpu()\n",
    "        \n",
    "        pose_R = out['pose_b'][0][:, :3, :3].double().cpu()[0]\n",
    "        gt_R = data['target_pose'][:, :3, :3].double().cpu()[0]\n",
    "        \n",
    "        R = torch.matmul(gt_R, torch.inverse(pose_R))\n",
    "        s = np.linalg.norm([R[0, 1]-R[1, 0],\n",
    "                            R[1, 2]-R[2, 1],\n",
    "                            R[0, 2]-R[2, 0]])\n",
    "        c = np.trace(R) - 1\n",
    "        RE = np.arctan2(s, c)\n",
    "        \n",
    "        ate = compute_ate(gt_t, pose_t)\n",
    "        seq_10_ates.append(np.array(ate))\n",
    "        seq_10_res.append(RE)\n",
    "        \n",
    "seq_10_ates = np.array(seq_10_ates)\n",
    "seq_10_res = np.array(seq_10_res)\n",
    "print(\"\\n   Trajectory error: {:0.3f}, std: {:0.3f}\\n\".format(np.mean(seq_10_ates), np.std(seq_10_ates)))\n",
    "print(\"\\n   Rotation error: {:0.3f}, std: {:0.3f}\\n\".format(np.mean(seq_10_res), np.std(seq_10_res)))\n",
    "\n",
    "# save_path = os.path.join(opt.load_weights_folder, \"poses.npy\")\n",
    "# np.save(save_path, pred_poses)\n",
    "# print(\"-> Predictions saved to\", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00035503041380532165"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pose_errors(gt, pred):\n",
    "    RE = 0\n",
    "    for (current_gt, current_pred) in zip(gt, pred):\n",
    "        snippet_length = current_gt.shape[0]\n",
    "        scale_factor = torch.sum(current_gt[..., -1] * current_pred[..., -1]) / torch.sum(current_pred[..., -1] ** 2)\n",
    "        ATE = torch.norm((current_gt[..., -1] - scale_factor * current_pred[..., -1]).reshape(-1)).cpu().numpy()\n",
    "        R = current_gt[..., :3] @ current_pred[..., :3].transpose(-2, -1)\n",
    "        for gt_pose, pred_pose in zip(current_gt, current_pred):\n",
    "            # Residual matrix to which we compute angle's sin and cos\n",
    "            R = (gt_pose[:, :3] @ torch.inverse(pred_pose[:, :3])).cpu().numpy()\n",
    "            s = np.linalg.norm([R[0, 1]-R[1, 0],\n",
    "                                R[1, 2]-R[2, 1],\n",
    "                                R[0, 2]-R[2, 0]])\n",
    "            c = np.trace(R) - 1\n",
    "            # Note: we actually compute double of cos and sin, but arctan2 is invariant to scale\n",
    "            RE += np.arctan2(s, c)\n",
    "\n",
    "    return [ATE/snippet_length, RE/snippet_length]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SF",
   "language": "python",
   "name": "sf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
