{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "from models.SceneNetStereoJoint import SceneNetStereoJoint\n",
    "from datasets.kitti_2015_train import KITTI_2015_MonoSceneFlow_Full\n",
    "\n",
    "from losses_eval import Eval_SceneFlow_KITTI_Test, Eval_SceneFlow_KITTI_Train\n",
    "from losses import Loss_SceneFlow_SelfSup_JointStereo\n",
    "\n",
    "from augmentations import Augmentation_Resize_Only\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Args:\n",
    "    model_name='scenenet_joint'\n",
    "    cuda = True\n",
    "    use_bn = True\n",
    "    momentum = 0.9\n",
    "    beta = 0.999\n",
    "    weight_decay=0.0\n",
    "    use_mask = True\n",
    "    pose_lr_w = 1.0\n",
    "    mask_lr_w = 1.0\n",
    "    disp_lr_w = 1.0\n",
    "    pose_sm_w = 200\n",
    "    disp_smooth_w = 0.1\n",
    "    mask_reg_w = 0.2\n",
    "    static_cons_w = 0.0\n",
    "    mask_cons_w = 0.3\n",
    "    mask_sm_w = 0.1\n",
    "    flow_diff_thresh=1e-3\n",
    "    evaluation=True\n",
    "\n",
    "args = Args()\n",
    "\n",
    "model = SceneNetStereoJoint(args).cuda()\n",
    "state_dict = torch.load('pretrained/scenenet_joint/latest.ckpt')\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:]\n",
    "    new_state_dict[name] = v\n",
    "model.load_state_dict(new_state_dict)\n",
    "\n",
    "augmentation = Augmentation_Resize_Only(args).cuda()\n",
    "\n",
    "val_dataset = KITTI_2015_MonoSceneFlow_Full(args, root=\"/external/datasets/kitti2015/\")\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=1, pin_memory=False)\n",
    "\n",
    "loss = Eval_SceneFlow_KITTI_Train(args)\n",
    "train_loss = Loss_SceneFlow_SelfSup_JointStereo(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200 [00:03<10:56,  3.30s/it]\n"
     ]
    }
   ],
   "source": [
    "loss_dict_avg = {}\n",
    "\n",
    "for i, data in enumerate(tqdm(val_loader)):\n",
    "    with torch.no_grad():\n",
    "        # Get input and target tensor keys\n",
    "        input_keys = list(filter(lambda x: \"input\" in x, data.keys()))\n",
    "        target_keys = list(filter(lambda x: \"target\" in x, data.keys()))\n",
    "        tensor_keys = input_keys + target_keys\n",
    "        \n",
    "        # Possibly transfer to Cuda\n",
    "        for k, v in data.items():\n",
    "            if k in tensor_keys:\n",
    "                data[k] = v.cuda(non_blocking=True)\n",
    "                \n",
    "        aug_data = augmentation(data)\n",
    "        out = model(aug_data)\n",
    "        train_loss_dict = train_loss(out, data)\n",
    "        loss_dict = loss(out, data)\n",
    "        \n",
    "        if i == 1:\n",
    "            break\n",
    "        \n",
    "        for k, v in loss_dict.items():\n",
    "            if k not in loss_dict_avg:\n",
    "                loss_dict_avg[k] = v\n",
    "            else:\n",
    "                loss_dict_avg[k] += v\n",
    "\n",
    "pct_keys = ['d1', 'f1', 'd2', 'sf']\n",
    "for k, v in loss_dict_avg.items():\n",
    "    loss_dict_avg[k] = v / 200;\n",
    "    if k in pct_keys:\n",
    "        loss_dict_avg[k] = v * 100;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.inverse_warp import pose2flow\n",
    "from utils.sceneflow_util import projectSceneFlow2Flow\n",
    "from losses import _elementwise_epe\n",
    "\n",
    "k_l2_aug = data['input_k_l2_aug'].detach()\n",
    "aug_size = data['aug_size']\n",
    "\n",
    "flow = out['flow_b'][0].detach()\n",
    "pose = out['pose_b'][0].detach()\n",
    "disp_l2 = out['disp_l2'][0].detach()\n",
    "\n",
    "# disparity\n",
    "disp_l2 = out['disp_l2'][0].detach()\n",
    "_, _, h_dp, w_dp = flow.size()\n",
    "disp_l2 = disp_l2 * w_dp\n",
    "\n",
    "local_scale = torch.zeros_like(aug_size)\n",
    "local_scale[:, 0] = h_dp\n",
    "local_scale[:, 1] = w_dp\n",
    "\n",
    "pts2, k2_scale, depth = pixel2pts_ms_depth(\n",
    "    k_l2_aug, disp_l2, local_scale / aug_size)\n",
    "\n",
    "x = pose2flow(depth.squeeze(dim=1), None, k2_scale, torch.inverse(k2_scale), pose_mat=pose)\n",
    "y = projectSceneFlow2Flow(k2_scale, flow, disp_l2)\n",
    "_elementwise_epe(x, y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a828e7d32a9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdisp_l2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_l1_warp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_l1_warp_cam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;31m#, census_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mdisp_l2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_l1_warp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_l1_warp_cam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcensus_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_visuals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 4)"
     ]
    }
   ],
   "source": [
    "from utils.sceneflow_util import pts2pixel_ms, pts2pixel_pose_ms, pixel2pts_ms_depth, reconstructImg\n",
    "\n",
    "def compute_census_mask():\n",
    "    return census_mask\n",
    "def get_visuals(input_dict, output_dict, model_name='scenenet_joint'):\n",
    "\n",
    "    img_l1_aug = input_dict['input_l1_aug'].detach()\n",
    "    img_l2_aug = input_dict['input_l2_aug'].detach()\n",
    "    img_r2_aug = input_dict['input_r2_aug'].detach()\n",
    "    k_l2_aug = input_dict['input_k_l2_aug'].detach()\n",
    "    aug_size = input_dict['aug_size']\n",
    "\n",
    "    sf_b = output_dict['flow_b'][0].detach()\n",
    "    if model_name == 'scenenet_stereo':\n",
    "        pose = output_dict['pose_b'].detach()\n",
    "    else:\n",
    "        pose = output_dict['pose_b'][0].detach()\n",
    "\n",
    "    mask = output_dict['mask_l2'][0].detach()\n",
    "    census_mask = output_dict['census_masks_l2'][0].detach()\n",
    "\n",
    "    # disparity\n",
    "    disp_l2 = output_dict['disp_l2'][0].detach()\n",
    "    _, _, h_dp, w_dp = sf_b.size()\n",
    "    disp_l2 = disp_l2 * w_dp\n",
    "\n",
    "    # scene flow\n",
    "    local_scale = torch.zeros_like(aug_size)\n",
    "    local_scale[:, 0] = h_dp\n",
    "    local_scale[:, 1] = w_dp\n",
    "\n",
    "    pts2, k2_scale, depth = pixel2pts_ms_depth(\n",
    "        k_l2_aug, disp_l2, local_scale / aug_size)\n",
    "\n",
    "    _, _, coord2 = pts2pixel_ms(k2_scale, pts2, sf_b, [h_dp, w_dp])\n",
    "    img_l1_warp = reconstructImg(coord2, img_l1_aug)\n",
    "\n",
    "    # camera pose\n",
    "    if args.model_name == 'scenenet_joint':\n",
    "        _, coord2 = pts2pixel_pose_ms(k2_scale, pts2, None, [h_dp, w_dp], pose_mat=pose)\n",
    "    else:\n",
    "        _, coord2 = pts2pixel_pose_ms(k2_scale, pts2, pose, [h_dp, w_dp])\n",
    "        \n",
    "    img_l1_warp_cam = reconstructImg(coord2, img_l1_aug)\n",
    "    \n",
    "    disp_l2 = disp_l2.cpu().squeeze()\n",
    "    img_l1_warp = img_l1_warp.cpu().squeeze().permute(1, 2, 0)\n",
    "    img_l1_warp_cam = img_l1_warp_cam.cpu().squeeze().permute(1, 2, 0)\n",
    "    mask = mask.cpu().squeeze()\n",
    "    census_mask = census_mask.cpu().squeeze()\n",
    "    \n",
    "    return disp_l2, img_l1_warp, img_l1_warp_cam, mask, census_mask\n",
    "\n",
    "disp_l2, img_l1_warp, img_l1_warp_cam, mask, census_mask = get_visuals(data, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_l1 = data['input_l1_aug'].squeeze().permute(1, 2, 0).cpu()\n",
    "img_l2 = data['input_l2_aug'].squeeze().permute(1, 2, 0).cpu()\n",
    "img_r1 = data['input_r1_aug'].squeeze().permute(1, 2, 0).cpu()\n",
    "img_r2 = data['input_r2_aug'].squeeze().permute(1, 2, 0).cpu()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(16, 16))\n",
    "ax1.imshow(img_l2)\n",
    "ax1.title.set_text('L2')\n",
    "ax2.imshow(img_r2)\n",
    "ax2.title.set_text('R2')\n",
    "\n",
    "fig, (ax3, ax4) = plt.subplots(1,2, figsize=(16, 16))\n",
    "ax3.imshow(img_l1)\n",
    "ax3.title.set_text('L1')\n",
    "ax4.imshow(img_r1)\n",
    "ax4.title.set_text('R1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "plt.imshow(disp_l2, cmap='plasma')\n",
    "plt.title('disparity')\n",
    "plt.show()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(32, 32));\n",
    "ax1.imshow(img_l1_warp);\n",
    "ax1.title.set_text('scene flow warped')\n",
    "ax2.imshow(img_l1_warp_cam);\n",
    "ax2.title.set_text('pose warped')\n",
    "\n",
    "fig, (ax3, ax4) = plt.subplots(1,2, figsize=(32, 32));\n",
    "ax3.imshow(mask, cmap='gray');\n",
    "ax3.title.set_text('learned mask')\n",
    "ax4.imshow(census_mask, cmap='gray');\n",
    "ax4.title.set_text('census mask')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SF",
   "language": "python",
   "name": "sf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
