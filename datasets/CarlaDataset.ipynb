{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from sys import exit\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms.functional import to_tensor \n",
    "\n",
    "from common import kitti_crop_image_list, kitti_adjust_intrinsic\n",
    "\n",
    "class CarlaDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 args,\n",
    "                 images_root=None,\n",
    "                 flip_augmentations=True,\n",
    "                 preprocessing_crop=True,\n",
    "                 crop_size=[370, 1224],\n",
    "                 num_examples=-1,\n",
    "                 index_file=None):\n",
    "\n",
    "        self._args = args\n",
    "        self._seq_len = 1\n",
    "        self._flip_augmentations = flip_augmentations\n",
    "        self._preprocessing_crop = preprocessing_crop\n",
    "        self._crop_size = crop_size\n",
    "        \n",
    "        self._image_list = []\n",
    "        self._pose_list = []\n",
    "        self._intrinsics = torch.from_numpy(np.loadtxt(os.path.join(images_root, 'intrinsics.txt'))).float()\n",
    "        \n",
    "        view1 = os.path.join(images_root, 'left')\n",
    "        view2 = os.path.join(images_root, 'right')\n",
    "        poses = os.path.join(images_root, 'poses')\n",
    "        \n",
    "        ext_dict = {\n",
    "            'rgb': '.png',\n",
    "            'depth': '.tif',\n",
    "            'segmentation': '.png',\n",
    "            'poses': '.json'\n",
    "        }\n",
    "        \n",
    "        self._output_keys = [\n",
    "            'rgb_l1',\n",
    "            'rgb_l2',\n",
    "            'rgb_r1',\n",
    "            'rgb_r2',\n",
    "            'depth_l1',\n",
    "            'depth_l2',\n",
    "            'depth_r1',\n",
    "            'depth_r2',\n",
    "            'seg_l1',\n",
    "            'seg_l2',\n",
    "            'seg_r1',\n",
    "            'seg_r2',\n",
    "            'ego_trans',\n",
    "            'k_l',\n",
    "            'k_r'\n",
    "        ]\n",
    "        \n",
    "        num_imgs = len(os.listdir(os.path.join(view1, 'rgb')))\n",
    "        \n",
    "        for idx in range(num_imgs-1):\n",
    "            idx_tgt = str(idx+1)\n",
    "            idx = str(idx)\n",
    "            paths = []\n",
    "            file_num = '0' * (6 - len(idx)) + idx\n",
    "            file_num_tgt = '0' * (6 - len(idx_tgt)) + idx_tgt\n",
    "            for sensor, ext in ext_dict.items():\n",
    "                if sensor == 'poses':\n",
    "                    paths.append(os.path.join(images_root, sensor, f\"{sensor}_frame_{file_num+ext}\"))\n",
    "                    paths.append(os.path.join(images_root, sensor, f\"{sensor}_frame_{file_num_tgt+ext}\"))\n",
    "                else:\n",
    "                    for view in [view1, view2]:\n",
    "                        paths.append(os.path.join(view, sensor, f\"{sensor}_{file_num+ext}\"))\n",
    "                        paths.append(os.path.join(view, sensor, f\"{sensor}_{file_num_tgt+ext}\"))\n",
    "            self._image_list.append(paths)\n",
    "        \n",
    "        self._size = len(self._image_list)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        index %= self._size\n",
    "        \n",
    "        # [rgbL12, rgbR12, depthL12, depthR12, segL12, segR12, poseLR1, poseLR2] # 14 files\n",
    "        data = [imread(path) for path in self._image_list[index][:-2]]\n",
    "        h_orig, w_orig, _ = data[0].shape\n",
    "        input_im_size = torch.from_numpy(np.array([h_orig, w_orig])).float()\n",
    "        \n",
    "        # cropping \n",
    "        if self._preprocessing_crop:\n",
    "            # get starting positions\n",
    "            crop_height = self._crop_size[0]\n",
    "            crop_width = self._crop_size[1]\n",
    "            x = np.random.uniform(0, w_orig - crop_width + 1)\n",
    "            y = np.random.uniform(0, h_orig - crop_height + 1)\n",
    "            crop_info = [int(x), int(y), int(x + crop_width), int(y + crop_height)]\n",
    "\n",
    "            # cropping images and adjust intrinsic accordingly\n",
    "            img_list_np = kitti_crop_image_list(data, crop_info)\n",
    "            k_l1, k_r1 = kitti_adjust_intrinsic(self._intrinsics, self._intrinsics, crop_info)\n",
    "        data = [to_tensor(x) for x in data]\n",
    "        pose_1 = self.read_pose(self._image_list[index][-2])\n",
    "        pose_2 = self.read_pose(self._image_list[index][-1])\n",
    "        ego_trans = torch.from_numpy(np.dot(pose_2, np.linalg.inv(pose_1)))\n",
    "        data.extend([ego_trans, k_l1, k_r1])\n",
    "        output_dict = {key: val for key, val in zip(self._output_keys, data)}\n",
    "        return output_dict\n",
    "\n",
    "    def read_pose(self, path):\n",
    "        with open(path) as f:\n",
    "            d = json.load(f)\n",
    "            T = np.array(d['vehicle']['Transform'])\n",
    "        return T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SF",
   "language": "python",
   "name": "sf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
