{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "from datasets.kitti_raw_monosf import KITTI_Raw_EigenSplit_Train, KITTI_Raw_EigenSplit_Valid\n",
    "from datasets.kitti_2015_train import KITTI_2015_MonoSceneFlow_Full\n",
    "\n",
    "from models.Model import Model\n",
    "from models.JointModel import JointModel\n",
    "from losses import Loss\n",
    "\n",
    "from augmentations import Augmentation_Resize_Only\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "class Args:\n",
    "    cuda = True\n",
    "    use_bn = False\n",
    "    momentum = 0.9\n",
    "    beta = 0.999\n",
    "    weight_decay=0.0\n",
    "    use_mask = False\n",
    "    use_flow_mask = False\n",
    "    flow_min_w = 0.5\n",
    "    flow_reduce_mode='sum'\n",
    "    ssim_w = 0.85\n",
    "    sf_lr_w = 0.0\n",
    "    pose_lr_w = 0.0\n",
    "    mask_lr_w = 1.0\n",
    "    disp_lr_w = 1.0\n",
    "    disp_pts_w = 0.0\n",
    "    sf_pts_w = 0.2\n",
    "    sf_sm_w = 200\n",
    "    fb_w = 0.0\n",
    "    pose_sm_w = 200\n",
    "    pose_pts_w = 0.2\n",
    "    disp_sm_w = 0.2\n",
    "    disp_smooth_w = 0.1\n",
    "    mask_reg_w = 0.2\n",
    "    encoder_name=\"resnet\"\n",
    "    model_name='joint'\n",
    "    static_cons_w = 1.0\n",
    "    mask_cons_w = 0.2\n",
    "    mask_sm_w = 0.1\n",
    "    flow_diff_thresh=1e-3\n",
    "    evaluation=True\n",
    "    num_scales = 4\n",
    "    pt_encoder=True\n",
    "    do_pose_c2f=False\n",
    "    use_disp_min=False\n",
    "    flow_pts_w=0.2\n",
    "    flow_sm_w=200\n",
    "    use_static_mask=False\n",
    "    use_census_mask=False\n",
    "    batch_size=2\n",
    "\n",
    "args = Args()\n",
    "\n",
    "model = JointModel(args).cuda()\n",
    "\n",
    "state_dict = torch.load('pretrained/49.ckpt')['model']\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:]\n",
    "    new_state_dict[name] = v\n",
    "model.load_state_dict(new_state_dict)\n",
    "model = model.eval()\n",
    "\n",
    "del state_dict\n",
    "del new_state_dict\n",
    "\n",
    "augmentation = Augmentation_Resize_Only(args).cuda()\n",
    "loss = Loss(args).cuda()\n",
    "\n",
    "# val_dataset = KITTI_2015_MonoSceneFlow_Full(args, root='/external/datasets/kitti2015/')\n",
    "val_dataset = KITTI_Raw_EigenSplit_Train(args, root='/external/datasets/kitti_data_jpg/', flip_augmentations=False, num_examples=150)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload \n",
    "\n",
    "ins = []\n",
    "outs = []\n",
    "\n",
    "for i, data in enumerate(val_loader):\n",
    "    with torch.no_grad():\n",
    "        # Get input and target tensor keys\n",
    "        input_keys = list(filter(lambda x: \"input\" in x, data.keys()))\n",
    "        target_keys = list(filter(lambda x: \"target\" in x, data.keys()))\n",
    "        tensor_keys = input_keys + target_keys\n",
    "        \n",
    "        # Possibly transfer to Cuda\n",
    "        for k, v in data.items():\n",
    "            if k in tensor_keys:\n",
    "                data[k] = v.cuda(non_blocking=True)\n",
    "                \n",
    "        aug_data = augmentation(data)\n",
    "        out = model(aug_data)\n",
    "        ins.append(aug_data)\n",
    "        outs.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as tf\n",
    "from utils.flow import flow_to_png_middlebury\n",
    "from utils.loss_utils import _disp2depth_kitti_K\n",
    "from utils.helpers import BackprojectDepth, Project3D\n",
    "from utils.inverse_warp import pose2sceneflow\n",
    "\n",
    "depth_imgs = []\n",
    "pose_imgs = []\n",
    "sf_imgs = []\n",
    "img_l2s = []\n",
    "\n",
    "for (aug_data, out) in zip(ins, outs):\n",
    "    img_l2 = aug_data['input_l2'].cpu().detach()\n",
    "    disp_l2 = out['disps_l2_pp'][0].cpu().detach()\n",
    "    pose_b = out['pose_b'][0].cpu().detach()\n",
    "    flow_b = out['flows_b_pp'][0].cpu().detach()\n",
    "    K = aug_data['input_k_l1_aug'].cpu().detach()\n",
    "    inv_K = torch.inverse(K)\n",
    "\n",
    "    b, _, h, w = flow_b.shape\n",
    "\n",
    "    backproj = BackprojectDepth(b, h, w)\n",
    "    proj = Project3D(b, h, w)\n",
    "\n",
    "    disp_l2 = disp_l2 * w\n",
    "    depth_l2 = _disp2depth_kitti_K(disp_l2, K[:, 0, 0])\n",
    "    \n",
    "    depth_l2 = tf.interpolate(depth_l2, [h//2, w//2], align_corners=True, mode='bilinear')\n",
    "    \n",
    "    cmap = plt.get_cmap('plasma')\n",
    "    \n",
    "    depth_img = cmap(depth_l2[0].squeeze(dim=1).numpy().astype(np.uint8), bytes=True).squeeze()[:, :, :-1]\n",
    "    depth_imgs.append(depth_img)\n",
    "    \n",
    "    depth_img = cmap(depth_l2[1].squeeze(dim=1).numpy().astype(np.uint8), bytes=True).squeeze()[:, :, :-1]\n",
    "    depth_imgs.append(depth_img)\n",
    "\n",
    "    # pose\n",
    "    pose_flow = pose2sceneflow(depth_l2.squeeze(dim=1), None, inv_K, pose_mat=pose_b)\n",
    "    pose_flow = tf.interpolate(pose_flow, [h//2, w//2], align_corners=True, mode='bilinear')\n",
    "    \n",
    "    pose_img = flow_to_png_middlebury(pose_flow[0].numpy())\n",
    "    pose_imgs.append(pose_img)\n",
    "    pose_img = flow_to_png_middlebury(pose_flow[1].numpy())\n",
    "    pose_imgs.append(pose_img)\n",
    "\n",
    "    # sf\n",
    "    flow_b = tf.interpolate(flow_b, [h//2, w//2], align_corners=True, mode='bilinear')\n",
    "    \n",
    "    sf_img = flow_to_png_middlebury(flow_b[0].numpy())\n",
    "    sf_imgs.append(sf_img)\n",
    "    sf_img = flow_to_png_middlebury(flow_b[1].numpy())\n",
    "    sf_imgs.append(sf_img)\n",
    "    \n",
    "    img = tf.interpolate(img_l2, [h//2, w//2], align_corners=True, mode='bilinear').permute(0, 2, 3, 1)\n",
    "    \n",
    "    img_l2s.append((img[0].numpy() * 255).astype(np.uint8))\n",
    "    img_l2s.append((img[1].numpy() * 255).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import imageio\n",
    "\n",
    "num_frames = len(depth_imgs)\n",
    "fps = 30 \n",
    "\n",
    "frames = []\n",
    "for (i, d, p, s) in zip(img_l2s, depth_imgs, pose_imgs, sf_imgs):\n",
    "    frame = np.zeros((256, 832, 3)).astype(np.uint8)\n",
    "    frame[0*128: 1*128, 0*416: 1*416, :] = i\n",
    "    frame[1*128: 2*128, 0*416: 1*416, :] = d\n",
    "    frame[0*128: 1*128, 1*416: 2*416, :] = p\n",
    "    frame[1*128: 2*128, 1*416: 2*416, :] = s\n",
    "    frames.append(frame)\n",
    "\n",
    "imageio.mimwrite('frames_ds.gif', frames, format='gif', fps=fps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SF",
   "language": "python",
   "name": "sf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
